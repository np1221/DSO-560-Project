{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"> <span style=\"color:firebrick\"> <font size=\"5\"> <b> USC Marshall School of Business </b> </font> </p> </span> \n",
    "\n",
    "<p style=\"text-align: center;\"> <b> <font font size=\"5\"> DSO 560 - Final Project </p> </b></font>\n",
    "\n",
    "<p style=\"text-align: center;\"> <b> Spring 2021 </b> </p>\n",
    "\n",
    "## <span style=\"color:black\"> <font size=\"3\">Group Maroon: Andrew Chuang, Janicia Chang, Ningchuan Peng, Zijing Wu</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T08:54:16.097266Z",
     "start_time": "2021-05-11T08:54:16.084263Z"
    }
   },
   "outputs": [],
   "source": [
    "# import relavent libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Classification\n",
    "\n",
    "### Executive Summary\n",
    "- For this question, we build a classification model to predict the brands that the products belong to. \n",
    "- We only use the top 30 brands and labeled other brand as \"Other\".\n",
    "- We utilize all of the columns that contain relevant text for prediction.\n",
    "- Basic preprocessing: Lower case, Stopwords removal, Lemmatization with part of speech tagging.\n",
    "- Two different vectorization techniques are tried: Document Embedding and TF-IDF Vectorization.\n",
    "- XGBoosting classifier is used for modeling and 5-fold cross-validation is used to evaluate different vectorization methods.\n",
    "- TF-IDF Vectorization technique achieves higher accuracy and is chosen as our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:51:59.223663Z",
     "start_time": "2021-05-11T07:51:49.603994Z"
    }
   },
   "outputs": [],
   "source": [
    "# read datasets\n",
    "product_df = pd.read_excel('Behold+product+data+04262021.xlsx', encoding = 'latin1') \n",
    "additional_tags = pd.read_csv('usc_additional_tags USC.csv', encoding = 'latin1')\n",
    "outfit_df = pd.read_csv('outfit_combinations USC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:52:08.498598Z",
     "start_time": "2021-05-11T07:52:08.363567Z"
    }
   },
   "outputs": [],
   "source": [
    "# group additional tags by product_id\n",
    "tags = additional_tags.groupby(['product_id']).agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T16:09:53.364248Z",
     "start_time": "2021-05-07T16:09:53.130181Z"
    }
   },
   "outputs": [],
   "source": [
    "# join additional tags with products dataset using product_id\n",
    "df = pd.merge(product_df, tags, on = 'product_id', how = 'left')\n",
    "df = df.fillna('Unknown').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T16:10:09.733127Z",
     "start_time": "2021-05-07T16:10:09.697099Z"
    }
   },
   "outputs": [],
   "source": [
    "# filterout the top 30 brands and label other brands as 'Other'\n",
    "top_30 = df['brand'].value_counts().nlargest(30).keys()\n",
    "df['brand_top30'] = df['brand'].apply(lambda x: x if x in top_30 else 'Other')\n",
    "df['product_active'] = df['product_active'].apply(lambda x: 1 if True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T16:10:37.613717Z",
     "start_time": "2021-05-07T16:10:35.057131Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a new column that contains all relavent text\n",
    "df['text'] = df[['brand_category', 'name', 'details', 'description', 'attribute_value']].agg(' '.join, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T16:10:40.094842Z",
     "start_time": "2021-05-07T16:10:39.975717Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert all text into lower case\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T16:11:54.089077Z",
     "start_time": "2021-05-07T16:11:54.078074Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to lemmatize sentences with Part of Speech tagging\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T16:11:56.201004Z",
     "start_time": "2021-05-07T16:11:56.188993Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a funtion to remove stopwords and lemmatize sentence\n",
    "def text_cleaning(x):\n",
    "    words = x.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in stopwords.words('english') + ['unknown']:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_text = \" \".join(new_words)\n",
    "    lemmatized_text = lemmatize_sentence(cleaned_text)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T16:35:47.923821Z",
     "start_time": "2021-05-07T16:11:58.005075Z"
    }
   },
   "outputs": [],
   "source": [
    "# creat a new column after text cleaning \n",
    "df['cleaned_text'] = df['text'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T08:12:14.825449Z",
     "start_time": "2021-05-11T08:12:00.791914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>name</th>\n",
       "      <th>details</th>\n",
       "      <th>created_at</th>\n",
       "      <th>brand_canonical_url</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_description</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_active</th>\n",
       "      <th>product_color_id</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>brand_top30</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01EX0PN4J9WRNZH5F93YEX6QAF</td>\n",
       "      <td>Two</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Khadi Stripe Shirt-our signature shirt</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2021-01-27 01:17:19.305 UTC</td>\n",
       "      <td>https://two-nyc.myshopify.com/products/white-k...</td>\n",
       "      <td>Our signature khadi shirt\\navailable in black ...</td>\n",
       "      <td>Our signature khadi shirt\\n\\navailable in blac...</td>\n",
       "      <td>Khadi Stripe Shirt-our signature shirt</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Other</td>\n",
       "      <td>unknown khadi stripe shirt-our signature shirt...</td>\n",
       "      <td>khadi stripe shirt-our signature shirt signatu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01F0C4SKZV6YXS3265JMC39NXW</td>\n",
       "      <td>Collina Strada</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>RUFFLE MARKET DRESS LOOPY PINK SISTINE TOMATO</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2021-03-09 18:43:10.457 UTC</td>\n",
       "      <td>https://collina-strada-2.myshopify.com/product...</td>\n",
       "      <td>Mid-length dress with ruffles and adjustable s...</td>\n",
       "      <td>Mid-length dress with ruffles and adjustable s...</td>\n",
       "      <td>RUFFLE MARKET DRESS LOOPY PINK SISTINE TOMATO</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Collina Strada</td>\n",
       "      <td>unknown ruffle market dress loopy pink sistine...</td>\n",
       "      <td>ruffle market dress loopy pink sistine tomato ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01EY4Y1BW8VZW51BWG5VZY82XW</td>\n",
       "      <td>Cariuma</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>IBI Slip On Raw Red Knit Sneaker Women</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2021-02-10 02:58:59.591 UTC</td>\n",
       "      <td>https://cariuma.myshopify.com/products/ibi-sli...</td>\n",
       "      <td>IBI Slip On Raw Red Knit Sneaker Women</td>\n",
       "      <td>IBI Slip On Raw Red Knit Sneaker Women</td>\n",
       "      <td>IBI Slip On Raw Red Knit Sneaker Women</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Other</td>\n",
       "      <td>unknown ibi slip on raw red knit sneaker women...</td>\n",
       "      <td>ibi slip raw red knit sneaker woman ibi slip r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id           brand brand_category  \\\n",
       "0  01EX0PN4J9WRNZH5F93YEX6QAF             Two        Unknown   \n",
       "1  01F0C4SKZV6YXS3265JMC39NXW  Collina Strada        Unknown   \n",
       "2  01EY4Y1BW8VZW51BWG5VZY82XW         Cariuma        Unknown   \n",
       "\n",
       "                                            name  details  \\\n",
       "0         Khadi Stripe Shirt-our signature shirt  Unknown   \n",
       "1  RUFFLE MARKET DRESS LOOPY PINK SISTINE TOMATO  Unknown   \n",
       "2         IBI Slip On Raw Red Knit Sneaker Women  Unknown   \n",
       "\n",
       "                    created_at  \\\n",
       "0  2021-01-27 01:17:19.305 UTC   \n",
       "1  2021-03-09 18:43:10.457 UTC   \n",
       "2  2021-02-10 02:58:59.591 UTC   \n",
       "\n",
       "                                 brand_canonical_url  \\\n",
       "0  https://two-nyc.myshopify.com/products/white-k...   \n",
       "1  https://collina-strada-2.myshopify.com/product...   \n",
       "2  https://cariuma.myshopify.com/products/ibi-sli...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Our signature khadi shirt\\navailable in black ...   \n",
       "1  Mid-length dress with ruffles and adjustable s...   \n",
       "2             IBI Slip On Raw Red Knit Sneaker Women   \n",
       "\n",
       "                                   brand_description  \\\n",
       "0  Our signature khadi shirt\\n\\navailable in blac...   \n",
       "1  Mid-length dress with ruffles and adjustable s...   \n",
       "2             IBI Slip On Raw Red Knit Sneaker Women   \n",
       "\n",
       "                                      brand_name  product_active  \\\n",
       "0         Khadi Stripe Shirt-our signature shirt               1   \n",
       "1  RUFFLE MARKET DRESS LOOPY PINK SISTINE TOMATO               1   \n",
       "2         IBI Slip On Raw Red Knit Sneaker Women               1   \n",
       "\n",
       "  product_color_id attribute_name attribute_value     brand_top30  \\\n",
       "0          Unknown        Unknown         Unknown           Other   \n",
       "1          Unknown        Unknown         Unknown  Collina Strada   \n",
       "2          Unknown        Unknown         Unknown           Other   \n",
       "\n",
       "                                                text  \\\n",
       "0  unknown khadi stripe shirt-our signature shirt...   \n",
       "1  unknown ruffle market dress loopy pink sistine...   \n",
       "2  unknown ibi slip on raw red knit sneaker women...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  khadi stripe shirt-our signature shirt signatu...  \n",
       "1  ruffle market dress loopy pink sistine tomato ...  \n",
       "2  ibi slip raw red knit sneaker woman ibi slip r...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_excel('cleaned.xlsx')\n",
    "# the text cleaning process is time consuming, run this cell to load the cleaned df directly\n",
    "df = pd.read_excel('cleaned.xlsx')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:19:45.026479Z",
     "start_time": "2021-05-10T15:10:45.839156Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a pre-trained word embeddings in spacy and apply to the dataset\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = df['cleaned_text'].astype(str).apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:19:57.445276Z",
     "start_time": "2021-05-10T15:19:45.028480Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the document embeddings \n",
    "# each document is represented as 1*300 vector\n",
    "emb_array = np.array(list(doc.apply(lambda x: list(x.vector))))\n",
    "embeddings = pd.DataFrame(emb_array, columns = range(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:58:03.435607Z",
     "start_time": "2021-05-10T15:20:21.338146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classification: \n",
      "\n",
      "Classification error of 10-folds:  [0.06372749 0.06201614 0.06258659 0.06633526 0.06283107]\n",
      "Mean classification error: 0.0634993073099176\n"
     ]
    }
   ],
   "source": [
    "# use 5-fold CV to evaluate model performance\n",
    "kfolds_classification = StratifiedKFold(n_splits = 5, random_state = 0, shuffle = True) \n",
    "# fit a xgboost classification model with document embeddings as x and brand names as y\n",
    "xgb_classification = xgb.XGBClassifier(eval_metric = 'merror')\n",
    "xgb_accuracy_cv = cross_val_score(xgb_classification, embeddings, df['brand_top30'], cv = kfolds_classification)\n",
    "print(\"Word Embeddings: \\n\")\n",
    "print(\"Classification error of 5-folds: \",1-xgb_accuracy_cv)\n",
    "print(\"Mean classification error:\",1-np.mean(xgb_accuracy_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T07:03:02.080114Z",
     "start_time": "2021-05-10T07:02:57.338017Z"
    }
   },
   "outputs": [],
   "source": [
    "# using tf-idf vectorizer to vectorize the dataset\n",
    "idf_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 max_features=1000,\n",
    "                                 min_df=5)\n",
    "tfidf = idf_vectorizer.fit_transform(df['cleaned_text'].astype(str))\n",
    "y = df['brand_top30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T14:05:49.498733Z",
     "start_time": "2021-05-10T13:58:29.832812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classification: \n",
      "\n",
      "Classification error of 10-folds:  [0.05574118 0.05696357 0.05631163 0.05834895 0.05345938]\n",
      "Mean classification error: 0.05616494173254016\n"
     ]
    }
   ],
   "source": [
    "# fit a xgboost classification model with tfidf scores as x and brand names as y\n",
    "xgb_classification = xgb.XGBClassifier(eval_metric = 'merror')\n",
    "xgb_accuracy_cv = cross_val_score(xgb_classification, tfidf, y, cv = kfolds_classification)\n",
    "print(\"TF-IDF Vectorization: \\n\")\n",
    "print(\"Classification error of 5-folds: \",1-xgb_accuracy_cv)\n",
    "print(\"Mean classification error:\",1-np.mean(xgb_accuracy_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Reflection\n",
    "- In general, the performance of the two models are very close and beat our expectations. We were able to achieve a mean classfication error of 5.6% with an xgboost classification model without any hyperparameters adjustment.\n",
    "- If we have more time and computating power in the future, we will try deep learning models such as RNN and LSTM to see if we can further improve our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Recommendation\n",
    "\n",
    "### Executive Summary\n",
    "To solve the problem of recommending an outfit to a customer, our group has come up with an algorithm:\n",
    "- We use Regex to classify the products into five categories: Top, Bottom, One piece, Shoes, and Accessory. \n",
    "- We get document embeddings of the query. \n",
    "- We calculate the cosine distance between the query and each product and find the product with the lowest cosine distance.\n",
    "- If the product is in the expert defined outfit dataset, we will use the outfit combination directly.\n",
    "- If the product is not in the outfit dataset, we will use the products with the lowest cosine distance in each category to form an outfit combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T08:39:49.781558Z",
     "start_time": "2021-05-11T08:39:49.772557Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a funtion to replace words\n",
    "def replace(title, word, reword):\n",
    "    count = 0\n",
    "    if isinstance(title, str):\n",
    "        title = re.sub(word, reword, title, flags=re.IGNORECASE)\n",
    "    return title\n",
    "\n",
    "# create a function to find the category\n",
    "def define_cate(title, word):\n",
    "    count = 'None'\n",
    "    if isinstance(title, str):\n",
    "        count = re.findall(word, title, re.IGNORECASE)\n",
    "        if len(count) == 0:\n",
    "            count = 'None'\n",
    "        else:\n",
    "            count = count[-1]\n",
    "    return count.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T08:40:00.162567Z",
     "start_time": "2021-05-11T08:39:59.283737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None         30598\n",
       "Bottom       12279\n",
       "Top          11985\n",
       "Shoe          2851\n",
       "One piece     2094\n",
       "Accessory     1548\n",
       "Name: product_category, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create words for replacement\n",
    "top_replace = r'\\b(top|shirt|t-shirt|hoodie|shirt|jacket|sweatshirt|coat|sweater|tee)\\b'\n",
    "bot_replace = r'\\b(bottom|shorts|dress|skirt|pants?|trousers?|jeans?|leg)\\b'\n",
    "one_replace = r'\\b(one piece|gown|swimsuit|overcoat|blouse|parka|blazer|jumpsuit|romper)\\b'\n",
    "shoe_replace = r'\\b(shoes?|heels?|flip flops?|boots?|sneakers?|sandals?|loafers?|flats?|mules?)\\b'\n",
    "acce_replace = r'\\b(handbag|bag|backpack|purse|tote|hat)\\b'\n",
    "\n",
    "# replace the words in different categories\n",
    "df['name_cleaned'] = df['name'].apply(lambda x: x.lower())\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, top_replace, r'TOP'))\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, bot_replace, r'Bottom'))\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, one_replace, r'One Piece'))\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, shoe_replace, r'Shoe'))\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, acce_replace, r'Accessory'))\n",
    "\n",
    "# classify the products to 5 big categories\n",
    "big_category = r'\\bTop|Bottom|One Piece|Shoe|Accessory\\b'\n",
    "df['product_category'] = df['name_cleaned'].apply(lambda x: define_cate(x, big_category))\n",
    "df['product_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above value counts we can see that there are still about 30000 products without a defined category. This is because we only used the name cloumn to do our regex matching. Since we will use product categories for our recommendations, we cannot afford miscategorizing a top as a bottom. Hence, we made the decision to optimize for precision and sacrafice recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:48:13.856993Z",
     "start_time": "2021-05-11T07:48:13.843989Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a function for giving back the outfit \n",
    "def give_back_result(product_id, cos_dis):\n",
    "    \n",
    "    # if the product is in the outfit combination dataset\n",
    "    if product_id in list(outfit_df['product_id']):\n",
    "        print('We find an outfit.')\n",
    "        \n",
    "        # get the best outfit combination\n",
    "        outfit_id = list(outfit_df[outfit_df['product_id'] == product_id]['outfit_id'])[0]\n",
    "        final_outfit = outfit_df[outfit_df['outfit_id'] == outfit_id]\n",
    "        \n",
    "        # print out the final result\n",
    "        for i in range(final_outfit.shape[0]):\n",
    "            out_type = final_outfit.iloc[i, 2].capitalize()\n",
    "            out_brand = final_outfit.iloc[i, 3]\n",
    "            out_prod = final_outfit.iloc[i, 4]\n",
    "            out_id = final_outfit.iloc[i, 1]\n",
    "            print(f'{out_type}: {out_brand} {out_prod} ({out_id})')\n",
    "    \n",
    "    # if the product is not in the outfit combination dataset\n",
    "    else:\n",
    "        print(\"We use products combination instead of an outfit.\")\n",
    "        \n",
    "        # get the products with the lowest cosine distance in 5 categories\n",
    "        df['cos_dis'] = cos_dis.values()\n",
    "        big_cate_list = ['Top', 'Bottom', 'One piece', 'Shoe', 'Accessory']\n",
    "        best_comb = {}\n",
    "        for i in range(len(big_cate_list)):\n",
    "            cate = df[df['product_category']== big_cate_list[i]].\\\n",
    "                   sort_index().sort_values('cos_dis', kind='mergesort').reset_index(drop=True)        \n",
    "            best_comb[cate.loc[0, 'product_category']] = [cate.loc[0, 'cos_dis'], \n",
    "                                                          cate.loc[0, 'brand'],\n",
    "                                                          cate.loc[0, 'name'],\n",
    "                                                          cate.loc[0, 'product_id']]\n",
    "            \n",
    "        # choose one piece or top and bottom based on the avearge cosine distance\n",
    "        if best_comb['One piece'][0] > (best_comb['Top'][0]+best_comb['Bottom'][0])/2:\n",
    "            del best_comb[\"One piece\"]\n",
    "        else:\n",
    "            del best_comb[\"Top\"]\n",
    "            del best_comb[\"Bottom\"]\n",
    "        \n",
    "        # print out the final result\n",
    "        for key in best_comb.keys():\n",
    "            print(f'{key}: {best_comb[key][1]} {best_comb[key][2]} ({best_comb[key][3]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:48:16.633645Z",
     "start_time": "2021-05-11T07:48:16.627645Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a function for searching for products\n",
    "def search(query):\n",
    "    \n",
    "    # get document embeddings of the query\n",
    "    query_nlp = nlp(query).vector\n",
    "    \n",
    "    # calculate the cosine distance between query and other products\n",
    "    cos_dis = {}\n",
    "    for i in range(embeddings.shape[0]):\n",
    "        cos_dis[i] = cosine(embeddings.loc[i,], query_nlp) \n",
    "    \n",
    "    # find the product with the lowest cosine distance\n",
    "    product_id = df.iloc[min(cos_dis, key=cos_dis.get),0]\n",
    "    \n",
    "    # run the function for giving back the outfit \n",
    "    give_back_result(product_id, cos_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T08:54:09.000460Z",
     "start_time": "2021-05-11T08:53:57.837564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your search query: \n",
      "Example: slim fitting, straight leg pant with a center back zipper and slightly cropped leg\n",
      "\n",
      "We use products combination instead of an outfit.\n",
      "One piece: A.L.C. Joana Jumpsuit (01EDYCA0E5S1N651MTXRA7AX60)\n",
      "Shoe: Citizens of Humanity Emannuelle Slim Boot (01EB2DRYX1B0QGMRBC1D3GS1C9)\n",
      "Accessory: A.L.C. Sadie Handbag (01EDYE843YM31CHVY8ED79DQYW)\n"
     ]
    }
   ],
   "source": [
    "# run the final test, please input nothing for example\n",
    "test_query = input('Please input your search query: ')\n",
    "if test_query == '':\n",
    "    test_query = 'slim fitting, straight leg pant with a center back zipper and slightly cropped leg'\n",
    "    print('Example: slim fitting, straight leg pant with a center back zipper and slightly cropped leg')\n",
    "    print()\n",
    "search(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the query 'slim fitting, straight leg pant with a center back zipper and slightly cropped leg'. The product that has the lowest cosine distance with this query using document embeddings is not in the outfit combinations dataset. Thus, we get the products with the lowest cosine distance in each category. Since One piece has a lower average cosine distance than Top and Bottom, the function finally gives us a product combination of One piece, Shoes, and Accessory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
